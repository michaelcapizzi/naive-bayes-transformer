{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, ClassifierMixin, BaseEstimator\n",
    "from sklearn.preprocessing import FunctionTransformer, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SVM` with `NB` features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the NB-SVM classifier from [this paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) that has become a classic (and competitive) baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy dataset\n",
    "\n",
    "Working example borrowed from [Stanford IR online text](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "\n",
    "| docID | text                                | label     |\n",
    "|-------|-------------------------------------|-----------|\n",
    "| 1     | chinese beijing chinese             | CHINA     |\n",
    "| 2     | chinese chinese shanghai            | CHINA     |\n",
    "| 3     | chinese macao                       | CHINA     |\n",
    "| 4     | tokyo japan chinese                 | NOT_CHINA |\n",
    "| 5     | chinese chinese chinese tokyo japan | ???       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_one = \"chinese beijing chinese\"\n",
    "label_one = 1\n",
    "\n",
    "doc_two = \"chinese chinese shanghai\"\n",
    "label_two = 1\n",
    "\n",
    "doc_three = \"chinese macao\"\n",
    "label_three = 1\n",
    "\n",
    "doc_four = \"tokyo japan chinese\"\n",
    "label_four = 0\n",
    "\n",
    "all_docs = [doc_one, doc_two, doc_three, doc_four]\n",
    "all_labels = [label_one, label_two, label_three, label_four]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional `bernoulli` `vectorizer`\n",
    "\n",
    "The paper found binary features to be more effective than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_bernoulli = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "X_bernoulli = vectorizer_bernoulli.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beijing': 0, 'chinese': 1, 'japan': 2, 'macao': 3, 'shanghai': 4, 'tokyo': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bernoulli.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bernoulli.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NB` transformer\n",
    "\n",
    "This is a custom `transformer` that will use the Naive Bayes conditional probabilities as feature values.  See [the paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) for details.\n",
    "\n",
    "The key thing to recognize is that this method is built off of the assumption of a `binary` (only two labels) problem.  It can be expanded to `multiclass` utilizing the `one-v-all` approach, but this also requires a **different** transformation for **each** label, thus the inclusion of `label_of_interest` as an argument to the `NBTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Feature transformation to utilize NaiveBayes conditional probabilities.\n",
    "    From https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, all_labels, label_of_interest, smoothing_factor=1):\n",
    "        \"\"\"\n",
    "        :param all_labels: <list> of all labels in training set\n",
    "        :param label_of_interest: label considered `positive`\n",
    "        :param smoothing_factor: smoothing value to be applied to transformation\n",
    "        \"\"\"\n",
    "        self.labels = all_labels\n",
    "        self.label_of_interest = label_of_interest\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self._r = None\n",
    "        \n",
    "    def _get_positive_rows(self):\n",
    "        \"\"\"\n",
    "        Finds all rows of data considered to be the `label_of_interest`\n",
    "        :return: <list> of indexes\n",
    "        \"\"\"\n",
    "        # ensure labels as np array\n",
    "        try:\n",
    "            as_array = np.array(self.labels)\n",
    "        except:\n",
    "            as_array = self.labels\n",
    "        # get indices of label_of_interest\n",
    "        idxs = np.where(as_array == self.label_of_interest)[0]\n",
    "        return idxs  \n",
    "    \n",
    "    def _get_negative_rows(self):\n",
    "        \"\"\"\n",
    "        Finds all rows of data considered NOT to be the `label_of_interest`\n",
    "        :return: <list> of indexes\n",
    "        \"\"\"\n",
    "        # ensure labels as np array\n",
    "        try:\n",
    "            as_array = np.array(self.labels)\n",
    "        except:\n",
    "            as_array = self.labels\n",
    "        # get indices of label_of_interest\n",
    "        idxs = np.where(as_array != self.label_of_interest)[0]\n",
    "        return idxs\n",
    "    \n",
    "    def _get_p_not_p(self, feature_matrix):\n",
    "        \"\"\"\n",
    "        Builds the *unnormalized* `p` and `not_p` vector (representing the conditional probabilities)\n",
    "        :param feature_matrix: sparse representation of features\n",
    "        :param alpha: smoothing factor\n",
    "        :return: <tuple> (p, not_p)\n",
    "        \"\"\"\n",
    "        # find indices of p and not_p\n",
    "        positive_rows = self._get_positive_rows()\n",
    "        negative_rows = self._get_negative_rows()\n",
    "        # get summed vectors\n",
    "        positive_summed = np.zeros((1, feature_matrix.get_shape()[1]))\n",
    "        negative_summed = np.zeros((1, feature_matrix.get_shape()[1]))\n",
    "        for i in range(feature_matrix.get_shape()[0]):\n",
    "            if i in positive_rows:\n",
    "                positive_summed += feature_matrix.getrow(i).toarray()\n",
    "            else:\n",
    "                negative_summed += feature_matrix.getrow(i).toarray()\n",
    "        # smooth\n",
    "        p = positive_summed + self.smoothing_factor\n",
    "        not_p = negative_summed + self.smoothing_factor\n",
    "        return p, not_p\n",
    "\n",
    "    def _get_r(self, un_normalized_p, un_normalized_not_p):\n",
    "        \"\"\"\n",
    "        Builds the `r` vector representing the `log` ratio of *normalized* `p` to `not_p`\n",
    "        :param un_normalized_p: unnormalized conditional probability vector for `p` (output of `._get_p_not_p()`)\n",
    "        :param un_normalized_not_p: unnormalized conditional probability vector for ` not_p` (output of `._get_p_not_p()`)\n",
    "        :return: `r`\n",
    "        \"\"\"\n",
    "        # normalize\n",
    "        p = un_normalized_p / np.sum(un_normalized_p)\n",
    "        not_p = un_normalized_not_p / np.sum(un_normalized_not_p)\n",
    "        # calculate r\n",
    "        r = np.log(p/not_p)\n",
    "        return r\n",
    "\n",
    "#     def fit_transform(self, X, *_):\n",
    "#         \"\"\"\n",
    "#         Applies the transformation of features\n",
    "#         :param X: original `sparse feature matrix`\n",
    "#         :return transformed `sparse feature matrix`\n",
    "#         \"\"\"\n",
    "#         if not np.any(self._r):\n",
    "#             # this is being called during training, so must build self._r\n",
    "#             # get positive rows\n",
    "#             pos_idxs = self._get_positive_rows()\n",
    "#             # get negative rows\n",
    "#             neg_idxs = self._get_negative_rows()\n",
    "#             # get p, not_p\n",
    "#             _p, _not_p = self._get_p_not_p(X)\n",
    "#             # get r\n",
    "#             self._r = self._get_r(_p, _not_p)\n",
    "#         return sparse.csr_matrix(X.multiply(self._r))\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        \"\"\"\n",
    "        Applies the transformation of features\n",
    "        :param X: original `sparse feature matrix`\n",
    "        :return transformed `sparse feature matrix`\n",
    "        \"\"\"\n",
    "        if not np.any(self._r):\n",
    "            # this is being called during training, so must build self._r\n",
    "            # get positive rows\n",
    "            pos_idxs = self._get_positive_rows()\n",
    "            # get negative rows\n",
    "            neg_idxs = self._get_negative_rows()\n",
    "            # get p, not_p\n",
    "            _p, _not_p = self._get_p_not_p(X)\n",
    "            # get r\n",
    "            self._r = self._get_r(_p, _not_p)\n",
    "        return sparse.csr_matrix(X.multiply(self._r))\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_transformer = NBTransformer(all_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = vectorizer_bernoulli.fit_transform(all_docs)\n",
    "original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40546511,  0.40546511,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.40546511,  0.        ,  0.        ,  0.40546511,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.40546511,  0.        ,  0.40546511,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.40546511, -0.98082925,  0.        ,  0.        ,\n",
       "        -0.98082925]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_transformed = nb_transformer.transform(original_features)\n",
    "nb_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper found `squared loss` and `l2` penalizing to be most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(            \n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state = 1,         # to ensure reproducible results\n",
    "    class_weight='balanced',\n",
    "    dual=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 1,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(nb_transformed, np.array(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we interpolate the weights using the following equation: <br>\n",
    "    $w' = (1 - \\beta)\\bar{w} + \\beta w$ where $\\bar{w}$ is the mean magnitude of all the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17147871,  0.21355567,  0.7278366 ,  0.17147871,  0.17147871,\n",
       "         0.7278366 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolation_factor = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36394416,  0.36394416,  0.36394416,  0.36394416,  0.36394416,\n",
       "         0.36394416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_magnitude = np.sum(svm_clf.coef_)/svm_clf.coef_.shape[1]\n",
    "mean_magnitude_vector = np.full(svm_clf.coef_.shape, mean_magnitude)\n",
    "mean_magnitude_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3158278 ,  0.32634704,  0.45491727,  0.3158278 ,  0.3158278 ,\n",
       "         0.45491727]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_ = (1 - interpolation_factor) * mean_magnitude_vector + \\\n",
    "                                interpolation_factor * svm_clf.coef_\n",
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"chinese chinese chinese tokyo japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first must `vectorize` the test item into the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features_original = vectorizer_bernoulli.transform([test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the previously learned transformer to apply the NB-transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.40546511, -0.98082925,  0.        ,  0.        ,\n",
       "        -0.98082925]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = nb_transformer.transform(test_features_original)\n",
    "test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the test document to be in the `not China` class with relative confidence.  \n",
    "\n",
    "Note: This is opposite the prediction made in the Stanford IR worked example because they use `multinomial NB` and the presence of \"China\" three times outweighed the presence of the other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([-0.2333769]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict(test_features), svm_clf.decision_function(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, this transformation is built for `binary` classification.  The `multiclass` version requires the application of `one-v-all` to a traditional `SVM` classifier.  This is built below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NB_plus_Classifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Implementation of this paper (https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) \n",
    "    using one-v-rest for multiclass datasets\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 list_of_classes=[0,1],\n",
    "                 clf=LinearSVC(     \n",
    "                    loss='squared_hinge',\n",
    "                    penalty='l2',\n",
    "                    random_state = 1,         # to ensure reproducible results\n",
    "                    class_weight='balanced'\n",
    "                    ),\n",
    "                 interpolation_factor=0.25\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param list_of_classes: <list> of all possible classes\n",
    "        :param clf: instance of a `sklearn` `Classifier`\n",
    "        :param interpolation_factor: amount to interpolate NB and Linear Classifier\n",
    "        \"\"\"\n",
    "        self.list_of_classes = list_of_classes\n",
    "        self.multiclass, self.ovr_classifiers = self._build_ovr_classifiers(list_of_classes, clf)\n",
    "        self.interpolation_factor = interpolation_factor if interpolation_factor else 1.0\n",
    "        self.nb_transformers = {}\n",
    "        \n",
    "    def _build_ovr_classifiers(self, list_of_all_possible_classes, classifier_instance):\n",
    "        \"\"\"\n",
    "        Builds a classifier for each class in the 'one-v-all' approach\n",
    "        \"\"\"\n",
    "        all_clfs = OrderedDict()\n",
    "        for i in list_of_all_possible_classes:\n",
    "            all_clfs[i] = classifier_instance\n",
    "        if len(all_clfs) == 2:\n",
    "            # is just a binary problem!\n",
    "            return False, all_clfs[list_of_all_possible_classes[0]]\n",
    "        elif len(all_clfs) > 2:\n",
    "            return True, all_clfs\n",
    "        else:\n",
    "            raise Exception(\"only one label was provided!\")\n",
    "        \n",
    "    def _binarize_labels(self, y):\n",
    "        distinct_labels = list(set(y))\n",
    "        all_labels = label_binarize(y, distinct_labels).transpose()\n",
    "        labels_dict = {}\n",
    "        for i in range(len(distinct_labels)):\n",
    "            labels_dict[distinct_labels[i]] = all_labels[i]\n",
    "        return labels_dict\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if not self.multiclass:\n",
    "            # transform X\n",
    "            # considering last label in self.list_of_classes as the `positive`\n",
    "            self.nb_transformers = NBTransformer(y, self.list_of_classes[-1])\n",
    "            X_transformed = self.nb_transformers.transform(X)\n",
    "            # just handle like a \"vanilla\" sklearn classifier\n",
    "            self.ovr_classifiers.fit(X_transformed, y)\n",
    "            # update weights with interpolation\n",
    "            try:\n",
    "                self.ovr_classifiers.coef_ = self._interpolate(self.ovr_classifiers.coef_)\n",
    "            except:\n",
    "                warnings.warn(\n",
    "                    \"the classifier you instantiated with does not have an attribute `.coef_`\"\n",
    "                    \"interpolation will not occur\"\n",
    "                )\n",
    "        else:\n",
    "            # handle with 'one-v-rest' approach\n",
    "            # binarize labels\n",
    "            labels_dict = self._binarize_labels(y)\n",
    "            if labels_dict.keys() != self.ovr_classifiers.keys():\n",
    "                raise Exception(\n",
    "                    \"mismatch in labels during fit() and during class instantiation; {} != {}\".format(\n",
    "                        labels_dict.keys(), self.ovr_classifiers.keys()\n",
    "                    )\n",
    "                  )\n",
    "            for l, clf in self.ovr_classifiers.items():\n",
    "                # transform X for this particular label\n",
    "                self.nb_transformers[l] = NBTransformer(y, l)\n",
    "                X_transformed = self.nb_transformers[l].transform(X)\n",
    "                # fit individual classifier with transformed data\n",
    "                clf.fit(X_transformed, labels_dict[l])\n",
    "                # update weights with interpolation\n",
    "                try:\n",
    "                    self.ovr_classifiers.coef_ = self._interpolate(self.ovr_classifiers.coef_)\n",
    "                except:\n",
    "                    if l == list(self.ovr_classifiers.keys())[0]:\n",
    "                        warnings.warn(\n",
    "                            \"the classifier you instantiated with does not have an attribute `.coef_`\"\n",
    "                            \"interpolation will not occur\"\n",
    "                        )\n",
    "            \n",
    "    def _interpolate(self, coeffs):\n",
    "        # calculate mean magnitude\n",
    "        mean_magnitude = np.sum(coeffs)/coeffs.shape[1]\n",
    "        mean_magnitude_vector = np.full(coeffs.shape, mean_magnitude)\n",
    "        # build interpolated coeffs\n",
    "        interpolated_coeffs = (1 - self.interpolation_factor) * mean_magnitude_vector + \\\n",
    "                                self.interpolation_factor * coeffs\n",
    "        return interpolated_coeffs\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        if not self.multiclass:\n",
    "            # transform X\n",
    "            X_transformed = self.nb_transformers.transform(X)\n",
    "            try:\n",
    "                final_distance = self.ovr_classifiers.decision_function(X_transformed)\n",
    "                return final_distance\n",
    "            except:\n",
    "                raise Exception(\n",
    "                    \"the classifier you instantiated with does not have a method for `decision_function()`\"\n",
    "                )\n",
    "        else:\n",
    "            # handle with 'one-v-rest' approach\n",
    "            all_distances = None\n",
    "            for l, clf in self.ovr_classifiers.items():\n",
    "                # transform X\n",
    "                X_transformed = self.nb_transformers[l].transform(X)\n",
    "                try:\n",
    "                    distance = clf.decision_function(X_transformed)\n",
    "                except:\n",
    "                    raise Exception(\n",
    "                        \"the classifier you instantiated with does not have a method for `decision_function()`\"\n",
    "                    )\n",
    "                if np.any(all_distances):\n",
    "                    all_distances = np.vstack((all_distances, distance))\n",
    "                else:\n",
    "                    all_distances = distance\n",
    "            return np.array(all_distances)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if not self.multiclass:\n",
    "            # transform X\n",
    "            X_transformed = self.nb_transformers.transform(X)\n",
    "            return self.ovr_classifiers.predict(X_transformed)\n",
    "        else:\n",
    "            # get all boundary distances\n",
    "            all_distances = self.decision_function(X)\n",
    "            # return most positive (or least negative) margin\n",
    "            return np.argmax(all_distances, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `binary` test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above custom `classifier` can still be used in the `binary` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = NB_plus_Classifier()\n",
    "binary.fit(vectorizer_bernoulli.fit_transform(all_docs), np.array(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = vectorizer_bernoulli.transform([test_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.23337768]), array([0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.decision_function(test_features), binary.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `multiclass` test\n",
    "\n",
    "Here is a simple toy dataset to test the `multiclass` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obvious_docs = [\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "]\n",
    "\n",
    "obvious_labels = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    \"cat\",\n",
    "    \"pen\",\n",
    "    \"pencil\",\n",
    "    \"car\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traditional `bernoulli` `vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiclass_bernoulli_vectorizer = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "multiclass_original_features = multiclass_bernoulli_vectorizer.fit_transform(obvious_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/ipykernel/__main__.py:87: UserWarning:\n",
      "\n",
      "the classifier you instantiated with does not have an attribute `.coef_`interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiclass = NB_plus_Classifier([0,1,2])\n",
    "multiclass.fit(multiclass_bernoulli_vectorizer.fit_transform(obvious_docs), obvious_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_test_features = multiclass_bernoulli_vectorizer.transform(test_docs)\n",
    "multiclass_test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generates predictions for each class.  Below, each row is a classifier trained for a different label.  The values in the vector represent the margin of each data point from the decision boundary.\n",
    "\n",
    "Note: Since the second data point (\"pen\") was never seen in the training vocabulary, the classifiers all generate the same prediction, none of which are positive (implying a low level of confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.33105273 -0.46413668 -0.89943101 -1.23883467]\n",
      "1 [-0.89943229 -0.46413668  0.33105039 -1.23883467]\n",
      "2 [-0.89943229 -0.46413668 -0.89943101  0.95106629]\n"
     ]
    }
   ],
   "source": [
    "for i, row in zip([0,1,2], multiclass.decision_function(multiclass_test_features)):\n",
    "    print(i, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the \"most confident\" classifier for each data point, where \"most confident\" is the one with the largest positive margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on `20Newsgroups`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `alt.atheism` v. `talk.religion.misc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categories = None\n",
    "categories = [\n",
    "    'alt.atheism', 'talk.religion.misc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers', \n",
    "#     'footers', \n",
    "#     'quotes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 480)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24154091645392586"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<857x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136539 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<570x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86232 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NB_plus_Classifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n",
      "training ridge - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training perceptron - nb transformed\n",
      "training passive_aggressive - nb transformed\n",
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/ipykernel/__main__.py:62: UserWarning:\n",
      "\n",
      "the classifier you instantiated with does not have an attribute `.coef_`interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.76315789473684215),\n",
       "             ('sgd', 0.81929824561403508),\n",
       "             ('ridge', 0.78596491228070176),\n",
       "             ('perceptron', 0.83157894736842108),\n",
       "             ('passive_aggressive', 0.8403508771929824),\n",
       "             ('adaboost', 0.75964912280701757)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n",
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.77719298245614032),\n",
       "             ('sgd', 0.756140350877193),\n",
       "             ('ridge', 0.77017543859649118),\n",
       "             ('perceptron', 0.76666666666666672),\n",
       "             ('passive_aggressive', 0.77017543859649118),\n",
       "             ('random_forest', 0.77894736842105261),\n",
       "             ('adaboost', 0.75964912280701757),\n",
       "             ('bernoulli_nb', 0.83157894736842108),\n",
       "             ('multinomial_nb', 0.83684210526315794)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.79649122807017547),\n",
       "             ('sgd', 0.81228070175438594),\n",
       "             ('ridge', 0.81403508771929822),\n",
       "             ('perceptron', 0.7929824561403509),\n",
       "             ('passive_aggressive', 0.82105263157894737),\n",
       "             ('random_forest', 0.75438596491228072),\n",
       "             ('adaboost', 0.74210526315789471),\n",
       "             ('bernoulli_nb', 0.83157894736842108),\n",
       "             ('multinomial_nb', 0.83859649122807023)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"c3672d1d-4c92-4358-8d97-b2ef906f88ea\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c3672d1d-4c92-4358-8d97-b2ef906f88ea\", [{\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.7771929824561403, 0.756140350877193, 0.7701754385964912, 0.7666666666666667, 0.7701754385964912, 0.7789473684210526, 0.7596491228070176, 0.8315789473684211, 0.8368421052631579], \"name\": \"no_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.7964912280701755, 0.8122807017543859, 0.8140350877192982, 0.7929824561403509, 0.8210526315789474, 0.7543859649122807, 0.7421052631578947, 0.8315789473684211, 0.8385964912280702], \"name\": \"tfidf_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.7631578947368421, 0.8192982456140351, 0.7859649122807018, 0.8315789473684211, 0.8403508771929824, 0.0, 0.7596491228070176, 0.0, 0.0], \"name\": \"nb_transform\"}], {\"yaxis\": {\"range\": [0, 1]}, \"title\": \"Transformation comparison: AthR (2.9)\", \"barmode\": \"group\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: AthR (2.9)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `comp.graphics` v. `comp.windows.x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categories = None\n",
    "categories = [\n",
    "    'comp.graphics', 'comp.windows.x',\n",
    "#     'rec.sport.baseball', 'sci.crypt',\n",
    "#     'alt.atheism', 'talk.religion.misc', 'comp.graphics', sci.space',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers', \n",
    "#     'footers', \n",
    "#     'quotes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'comp.graphics', 1: 'comp.windows.x'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 584)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0152934161694984"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1177x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136868 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<784x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NB_plus_Classifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n",
      "training ridge - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training perceptron - nb transformed\n",
      "training passive_aggressive - nb transformed\n",
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/ipykernel/__main__.py:62: UserWarning:\n",
      "\n",
      "the classifier you instantiated with does not have an attribute `.coef_`interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.82908163265306123),\n",
       "             ('sgd', 0.85076530612244894),\n",
       "             ('ridge', 0.85969387755102045),\n",
       "             ('perceptron', 0.85331632653061229),\n",
       "             ('passive_aggressive', 0.8660714285714286),\n",
       "             ('adaboost', 0.81122448979591832)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n",
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.82397959183673475),\n",
       "             ('sgd', 0.8482142857142857),\n",
       "             ('ridge', 0.83418367346938771),\n",
       "             ('perceptron', 0.83290816326530615),\n",
       "             ('passive_aggressive', 0.82908163265306123),\n",
       "             ('random_forest', 0.81377551020408168),\n",
       "             ('adaboost', 0.81122448979591832),\n",
       "             ('bernoulli_nb', 0.81760204081632648),\n",
       "             ('multinomial_nb', 0.86352040816326525)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.8482142857142857),\n",
       "             ('sgd', 0.8571428571428571),\n",
       "             ('ridge', 0.85459183673469385),\n",
       "             ('perceptron', 0.84311224489795922),\n",
       "             ('passive_aggressive', 0.85459183673469385),\n",
       "             ('random_forest', 0.80867346938775508),\n",
       "             ('adaboost', 0.78061224489795922),\n",
       "             ('bernoulli_nb', 0.81760204081632648),\n",
       "             ('multinomial_nb', 0.86734693877551017)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"e5d061d9-3db5-45d3-b166-3a3fdce5a62b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e5d061d9-3db5-45d3-b166-3a3fdce5a62b\", [{\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.8239795918367347, 0.8482142857142857, 0.8341836734693877, 0.8329081632653061, 0.8290816326530612, 0.8137755102040817, 0.8112244897959183, 0.8176020408163265, 0.8635204081632653], \"name\": \"no_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.8482142857142857, 0.8571428571428571, 0.8545918367346939, 0.8431122448979592, 0.8545918367346939, 0.8086734693877551, 0.7806122448979592, 0.8176020408163265, 0.8673469387755102], \"name\": \"tfidf_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.8290816326530612, 0.8507653061224489, 0.8596938775510204, 0.8533163265306123, 0.8660714285714286, 0.0, 0.8112244897959183, 0.0, 0.0], \"name\": \"nb_transform\"}], {\"yaxis\": {\"range\": [0, 1]}, \"title\": \"Transformation comparison: XGraph (1.8)\", \"barmode\": \"group\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: XGraph (1.8)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rec.sport.baseball` v. `sci.crypt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categories = None\n",
    "categories = [\n",
    "    'rec.sport.baseball', 'sci.crypt',\n",
    "#     'alt.atheism', 'talk.religion.misc', 'comp.graphics', sci.space',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers', \n",
    "#     'footers', \n",
    "#     'quotes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'rec.sport.baseball', 1: 'sci.crypt'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595, 597)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0033557078469723042"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1192x21197 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 176313 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<793x21197 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 97031 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NB_plus_Classifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n",
      "training ridge - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training perceptron - nb transformed\n",
      "training passive_aggressive - nb transformed\n",
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/ipykernel/__main__.py:62: UserWarning:\n",
      "\n",
      "the classifier you instantiated with does not have an attribute `.coef_`interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.84489281210592682),\n",
       "             ('sgd', 0.96973518284993698),\n",
       "             ('ridge', 0.97856242118537196),\n",
       "             ('perceptron', 0.97982345523329129),\n",
       "             ('passive_aggressive', 0.98486759142496849),\n",
       "             ('adaboost', 0.9319041614123581)])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n",
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.95838587641866335),\n",
       "             ('sgd', 0.95586380832282469),\n",
       "             ('ridge', 0.94703656998738961),\n",
       "             ('perceptron', 0.95208070617906682),\n",
       "             ('passive_aggressive', 0.95964691046658257),\n",
       "             ('random_forest', 0.94955863808322827),\n",
       "             ('adaboost', 0.9319041614123581),\n",
       "             ('bernoulli_nb', 0.94703656998738961),\n",
       "             ('multinomial_nb', 0.98612862547288782)])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nextiva-pipeline/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:311: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.95208070617906682),\n",
       "             ('sgd', 0.96343001261034045),\n",
       "             ('ridge', 0.9709962168978562),\n",
       "             ('perceptron', 0.95586380832282469),\n",
       "             ('passive_aggressive', 0.97477931904161408),\n",
       "             ('random_forest', 0.94955863808322827),\n",
       "             ('adaboost', 0.91298865069356872),\n",
       "             ('bernoulli_nb', 0.94703656998738961),\n",
       "             ('multinomial_nb', 0.97982345523329129)])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"830f137d-58e3-466f-b588-ac56b8ccc96d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"830f137d-58e3-466f-b588-ac56b8ccc96d\", [{\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.9583858764186634, 0.9558638083228247, 0.9470365699873896, 0.9520807061790668, 0.9596469104665826, 0.9495586380832283, 0.9319041614123581, 0.9470365699873896, 0.9861286254728878], \"name\": \"no_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.9520807061790668, 0.9634300126103404, 0.9709962168978562, 0.9558638083228247, 0.9747793190416141, 0.9495586380832283, 0.9129886506935687, 0.9470365699873896, 0.9798234552332913], \"name\": \"tfidf_transform\"}, {\"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"type\": \"bar\", \"y\": [0.8448928121059268, 0.969735182849937, 0.978562421185372, 0.9798234552332913, 0.9848675914249685, 0.0, 0.9319041614123581, 0.0, 0.0], \"name\": \"nb_transform\"}], {\"yaxis\": {\"range\": [0, 1]}, \"title\": \"Transformation comparison: BbCrypt (0.5)\", \"barmode\": \"group\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: BbCrypt (0.5)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
