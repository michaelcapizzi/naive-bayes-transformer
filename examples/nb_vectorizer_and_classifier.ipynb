{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SVM` with `NB` features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements shows how to use the NB-SVM classifier from [this paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) that has become a classic (and competitive) baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy dataset (binary)\n",
    "\n",
    "Working example borrowed from [Stanford IR online text](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "\n",
    "| docID | text                                | label     |\n",
    "|-------|-------------------------------------|-----------|\n",
    "| 1     | chinese beijing chinese             | CHINA     |\n",
    "| 2     | chinese chinese shanghai            | CHINA     |\n",
    "| 3     | chinese macao                       | CHINA     |\n",
    "| 4     | tokyo japan chinese                 | NOT_CHINA |\n",
    "| 5     | chinese chinese chinese tokyo japan | ???       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_one = \"chinese beijing chinese\"\n",
    "label_one = 1\n",
    "\n",
    "doc_two = \"chinese chinese shanghai\"\n",
    "label_two = 1\n",
    "\n",
    "doc_three = \"chinese macao\"\n",
    "label_three = 1\n",
    "\n",
    "doc_four = \"tokyo japan chinese\"\n",
    "label_four = 0\n",
    "\n",
    "all_docs = [doc_one, doc_two, doc_three, doc_four]\n",
    "all_labels = [label_one, label_two, label_three, label_four]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0, os.path.join(parentdir, 'nb_transformer'))\n",
    "from nb_transformer import NaiveBayesTransformer\n",
    "from nb_enhanced_classifier import NaiveBayesEnhancedClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional `bernoulli` `vectorizer`\n",
    "\n",
    "First let's `vectorize` the documents in the training set using `CountVectorizer()`. <br>\n",
    "The paper found binary features to be more effective than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_bernoulli = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "X_bernoulli = vectorizer_bernoulli.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beijing': 0, 'chinese': 1, 'japan': 2, 'macao': 3, 'shanghai': 4, 'tokyo': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bernoulli.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bernoulli.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NaiveBayesTransformer`\n",
    "\n",
    "This is a custom `transformer` that will use the Naive Bayes conditional probabilities as feature values.  See [the paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) for details.\n",
    "\n",
    "The key thing to recognize is that this method is built off of the assumption of a `binary` (only two labels) problem.  It can be expanded to `multiclass` utilizing the `one-v-all` approach, but this also requires a **different** transformation for **each** label, thus the inclusion of `label_of_interest` as an argument to the `NaiveBayesTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_transformer = NaiveBayesTransformer(all_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = vectorizer_bernoulli.fit_transform(all_docs)\n",
    "original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4054651 ,  0.4054651 , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.        ,  0.4054651 ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.4054651 ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.98082924,  0.        ,  0.        ,\n",
       "        -0.98082924]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_transformed = nb_transformer.fit_transform(original_features)\n",
    "nb_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our features, let's train a classifier to predict `1` v. `0`. \n",
    "The paper found an `SVM` with `squared loss` and `l2` penalizing to be most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(            \n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state = 1,         # to ensure reproducible results\n",
    "    class_weight='balanced',\n",
    "    dual=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an instance of `NaiveBayesEnchancedClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 1,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    base_clf=svm_clf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it on our dataset by feeding the *original* vectorized features into `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(base_clf=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "     verbose=0),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.fit(original_features, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baked into `.fit()` are the following steps:\n",
    " 1. `transform` the features using the `NaiveBayesTransformer`\n",
    " 2. Apply an interpolation of weights to the `.coef_` of the `classifier` using the formula below: \n",
    "  - $w' = (1 - \\beta)\\bar{w} + \\beta w$ where $\\bar{w}$ is the mean magnitude of all the weights\n",
    "  \n",
    "Below are a few cells showing the calculations that are done in `NaiveBayesTransformer._interpolate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3158278 ,  0.32634704,  0.45491727,  0.3158278 ,  0.3158278 ,\n",
       "         0.45491727]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolation_factor = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36394416,  0.36394416,  0.36394416,  0.36394416,  0.36394416,\n",
       "         0.36394416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_magnitude = np.sum(svm_clf.coef_)/svm_clf.coef_.shape[1]\n",
    "mean_magnitude_vector = np.full(svm_clf.coef_.shape, mean_magnitude)\n",
    "mean_magnitude_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35191507,  0.35454488,  0.38668744,  0.35191507,  0.35191507,\n",
       "         0.38668744]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_ = (1 - interpolation_factor) * mean_magnitude_vector + \\\n",
    "                                interpolation_factor * svm_clf.coef_\n",
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict on a new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"chinese chinese chinese tokyo japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first must `vectorize` the test item into the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features_original = vectorizer_bernoulli.transform([test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `NaiveBayesEnhancedClassifier.predict()` to (1) apply the `NB-transformation` and make our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.predict(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the \"confidence\" of this prediction using `NaiveBayesEnhancedClassifier.decision_function_predict_proba(test_features_original)` which will use `.decision_function()` or `.predict_proba()` of the original `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08810002])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.decision_function_predict_proba(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the test document to be in the `not China` class but not with much confidence.  \n",
    "\n",
    "Note: This is opposite the prediction made in the Stanford IR worked example because they use `multinomial NB` and the presence of \"China\" three times outweighed the presence of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `multiclass` test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, this transformation is built for `binary` classification.  The `multiclass` version requires the application of `one-v-all` to a traditional `SVM` classifier.  This is all done automatically within the `NaiveBayesEnhancedClassifier` `class`.  An example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obvious_docs = [\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "]\n",
    "\n",
    "obvious_labels = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    \"cat\",\n",
    "    \"pen\",\n",
    "    \"pencil\",\n",
    "    \"car\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traditional `bernoulli` `vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_bernoulli_vectorizer = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "multiclass_original_features = multiclass_bernoulli_vectorizer.fit_transform(obvious_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    # we'll use the default settings for `clf`\n",
    ")\n",
    "before = multiclass_example_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(base_clf=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_clf.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = multiclass_example_clf.get_params()\n",
    "before == after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project the test documents into the same feature space as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_test_features = multiclass_bernoulli_vectorizer.transform(test_docs)\n",
    "multiclass_test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's get predictions for each test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_clf.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the classifier is generating predictions for how likely the data point is in that class.  We can see these values by calling `NaiveBayesEnhancedClassifier.decision_function_predict_proba()`.\n",
    "\n",
    "Below, each row is a classifier trained for a different label.  The values in the vector represent the margin of each data point from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.48603642 -0.4641514  -0.98429487 -1.06914084]\n",
      "1 [-0.98429487 -0.4641514   0.48603642 -1.06914084]\n",
      "2 [-0.98429487 -0.4641514  -0.98429487  0.64103137]\n"
     ]
    }
   ],
   "source": [
    "for i, row in zip([0,1,2], multiclass_example_clf.decision_function_predict_proba(multiclass_test_features)):\n",
    "    print(i, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the \"most confident\" classifier for each data point, where \"most confident\" is the one with the largest *positive* (or, if they're all *negative*, the *smallest*) margin.  In this case, each column is a data point, so we take the `argmax` of each column.\n",
    "\n",
    "Note: Since the second data point (\"pen\") was never seen in the training vocabulary, the classifiers all generate the same prediction, none of which are positive (implying a low level of confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `classifier` without `.coef_` or `.decision_function()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NaiveBayesEnhancedClassifier` is built to wrap *any* `sklearn` classifier.  If the classifier does *not* use weights that are captured in the attribute, `.coef_`, then `interpolation` is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_example_no_decision_function = NaiveBayesEnhancedClassifier(\n",
    "    clf_=RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:187: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(clf_=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, if the `classifier` chosen does *not* have a `.decision_function()` method, then `.predict_proba()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:258: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0. ,  0.5],\n",
       "       [ 0.1,  0.5,  0.5,  0.5],\n",
       "       [ 0.1,  0.5,  0. ,  1. ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.decision_function_predict_proba(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each row still represents a `binary` classifier for a given label, and the vector values are the probabilities of the `positive` class.  We still choose the `argmax` in each column to determine our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:258: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on `20Newsgroups`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we attempt to recreate the `binary` classification problems in the experiments of the [original paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf).  Whenever provided, we use the same hyperparameters.\n",
    "\n",
    "We also compare different classifiers and the use of (1) no transformer or (2) `TfidfVectorizer()`.\n",
    "\n",
    "Note: It's likely that the data coming from `sklearn` for `20Newsgroups` is *not* exactly the same as the data used by the authors.  But the results are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `alt.atheism` v. `talk.religion.misc` (`AthR`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism', 'talk.religion.misc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 480)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<857x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136539 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<570x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86232 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n",
      "training ridge - nb transformed\n",
      "training perceptron - nb transformed\n",
      "training passive_aggressive - nb transformed\n",
      "training random_forest - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:162: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:162: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed random_forest\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.83508771929824566),\n",
       "             ('sgd', 0.79649122807017547),\n",
       "             ('ridge', 0.79122807017543861),\n",
       "             ('perceptron', 0.79473684210526319),\n",
       "             ('passive_aggressive', 0.83859649122807023),\n",
       "             ('random_forest', 0.77894736842105261),\n",
       "             ('adaboost', 0.75964912280701757)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n",
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:319: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.77719298245614032),\n",
       "             ('sgd', 0.756140350877193),\n",
       "             ('ridge', 0.77017543859649118),\n",
       "             ('perceptron', 0.76666666666666672),\n",
       "             ('passive_aggressive', 0.77017543859649118),\n",
       "             ('random_forest', 0.77894736842105261),\n",
       "             ('adaboost', 0.75964912280701757),\n",
       "             ('bernoulli_nb', 0.83157894736842108),\n",
       "             ('multinomial_nb', 0.83684210526315794)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:319: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n",
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.79649122807017547),\n",
       "             ('sgd', 0.81228070175438594),\n",
       "             ('ridge', 0.81403508771929822),\n",
       "             ('perceptron', 0.7929824561403509),\n",
       "             ('passive_aggressive', 0.82105263157894737),\n",
       "             ('random_forest', 0.75438596491228072),\n",
       "             ('adaboost', 0.74210526315789471),\n",
       "             ('bernoulli_nb', 0.83157894736842108),\n",
       "             ('multinomial_nb', 0.83859649122807023)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "no_transform",
         "type": "bar",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.7771929824561403,
          0.756140350877193,
          0.7701754385964912,
          0.7666666666666667,
          0.7701754385964912,
          0.7789473684210526,
          0.7596491228070176,
          0.8315789473684211,
          0.8368421052631579
         ]
        },
        {
         "name": "tfidf_transform",
         "type": "bar",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.7964912280701755,
          0.8122807017543859,
          0.8140350877192982,
          0.7929824561403509,
          0.8210526315789474,
          0.7543859649122807,
          0.7421052631578947,
          0.8315789473684211,
          0.8385964912280702
         ]
        },
        {
         "name": "nb_transform",
         "type": "bar",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.8350877192982457,
          0.7964912280701755,
          0.7912280701754386,
          0.7947368421052632,
          0.8385964912280702,
          0.7789473684210526,
          0.7596491228070176,
          0,
          0
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "title": "Transformation comparison: AthR (2.9)",
        "yaxis": {
         "range": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"b8fc08ba-87b9-4201-96b8-1e037cb04947\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"b8fc08ba-87b9-4201-96b8-1e037cb04947\", [{\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7771929824561403, 0.756140350877193, 0.7701754385964912, 0.7666666666666667, 0.7701754385964912, 0.7789473684210526, 0.7596491228070176, 0.8315789473684211, 0.8368421052631579], \"name\": \"no_transform\"}, {\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7964912280701755, 0.8122807017543859, 0.8140350877192982, 0.7929824561403509, 0.8210526315789474, 0.7543859649122807, 0.7421052631578947, 0.8315789473684211, 0.8385964912280702], \"name\": \"tfidf_transform\"}, {\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8350877192982457, 0.7964912280701755, 0.7912280701754386, 0.7947368421052632, 0.8385964912280702, 0.7789473684210526, 0.7596491228070176, 0.0, 0.0], \"name\": \"nb_transform\"}], {\"barmode\": \"group\", \"title\": \"Transformation comparison: AthR (2.9)\", \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"b8fc08ba-87b9-4201-96b8-1e037cb04947\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"b8fc08ba-87b9-4201-96b8-1e037cb04947\", [{\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7771929824561403, 0.756140350877193, 0.7701754385964912, 0.7666666666666667, 0.7701754385964912, 0.7789473684210526, 0.7596491228070176, 0.8315789473684211, 0.8368421052631579], \"name\": \"no_transform\"}, {\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7964912280701755, 0.8122807017543859, 0.8140350877192982, 0.7929824561403509, 0.8210526315789474, 0.7543859649122807, 0.7421052631578947, 0.8315789473684211, 0.8385964912280702], \"name\": \"tfidf_transform\"}, {\"type\": \"bar\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8350877192982457, 0.7964912280701755, 0.7912280701754386, 0.7947368421052632, 0.8385964912280702, 0.7789473684210526, 0.7596491228070176, 0.0, 0.0], \"name\": \"nb_transform\"}], {\"barmode\": \"group\", \"title\": \"Transformation comparison: AthR (2.9)\", \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: AthR (2.9)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `comp.graphics` v. `comp.windows.x` (`XGraph`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'comp.graphics', 'comp.windows.x',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'comp.graphics', 1: 'comp.windows.x'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 584)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1177x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136868 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<784x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "/home/mcapizzi/anaconda3/envs/nb_plus_svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'interpolation_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-422ae3c5d6cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2class_lookup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0minterpolation_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'interpolation_factor'"
     ]
    }
   ],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: XGraph (1.8)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rec.sport.baseball` v. `sci.crypt` (`BbCrypt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.sport.baseball', 'sci.crypt',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: BbCrypt (0.5)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `20Newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: 20Newsgroups',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
