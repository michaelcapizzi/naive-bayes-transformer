{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SVM` with `NB` features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements shows how to use the NB-SVM classifier from [this paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) that has become a classic (and competitive) baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy dataset (binary)\n",
    "\n",
    "Working example borrowed from [Stanford IR online text](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "\n",
    "| docID | text                                | label     |\n",
    "|-------|-------------------------------------|-----------|\n",
    "| 1     | chinese beijing chinese             | CHINA     |\n",
    "| 2     | chinese chinese shanghai            | CHINA     |\n",
    "| 3     | chinese macao                       | CHINA     |\n",
    "| 4     | tokyo japan chinese                 | NOT_CHINA |\n",
    "| 5     | chinese chinese chinese tokyo japan | ???       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one = \"chinese beijing chinese\"\n",
    "label_one = 1\n",
    "\n",
    "doc_two = \"chinese chinese shanghai\"\n",
    "label_two = 1\n",
    "\n",
    "doc_three = \"chinese macao\"\n",
    "label_three = 1\n",
    "\n",
    "doc_four = \"tokyo japan chinese\"\n",
    "label_four = 0\n",
    "\n",
    "all_docs = [doc_one, doc_two, doc_three, doc_four]\n",
    "all_labels = [label_one, label_two, label_three, label_four]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0, os.path.join(parentdir, 'nb_transformer'))\n",
    "from nb_transformer import NaiveBayesTransformer\n",
    "from nb_enhanced_classifier import NaiveBayesEnhancedClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional `bernoulli` `vectorizer`\n",
    "\n",
    "First let's `vectorize` the documents in the training set using `CountVectorizer()`. <br>\n",
    "The paper found binary features to be more effective than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bernoulli = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "X_bernoulli = vectorizer_bernoulli.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chinese': 1, 'beijing': 0, 'shanghai': 4, 'macao': 3, 'tokyo': 5, 'japan': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bernoulli.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bernoulli.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NaiveBayesTransformer`\n",
    "\n",
    "This is a custom `transformer` that will use the Naive Bayes conditional probabilities as feature values.  See [the paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) for details.\n",
    "\n",
    "The key thing to recognize is that this method is built off of the assumption of a `binary` (only two labels) problem.  It can be expanded to `multiclass` utilizing the `one-v-all` approach, but this also requires a **different** transformation for **each** label, thus the inclusion of `label_of_interest` as an argument to the `NaiveBayesTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_transformer = NaiveBayesTransformer(all_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = vectorizer_bernoulli.fit_transform(all_docs)\n",
    "original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4054651 ,  0.4054651 , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.        ,  0.4054651 ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.4054651 ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.98082924,  0.        ,  0.        ,\n",
       "        -0.98082924]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_transformed = nb_transformer.fit_transform(original_features)\n",
    "nb_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our features, let's train a classifier to predict `1` v. `0`. \n",
    "The paper found an `SVM` with `squared loss` and `l2` penalizing to be most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(            \n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state = 1,         # to ensure reproducible results\n",
    "    class_weight='balanced',\n",
    "    dual=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an instance of `NaiveBayesEnhancedClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 1,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    base_clf=svm_clf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it on our dataset by feeding the *original* vectorized features into `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(base_clf=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "     verbose=0),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.fit(original_features, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baked into `.fit()` are the following steps:\n",
    " 1. `transform` the features using the `NaiveBayesTransformer`\n",
    " 2. Apply an interpolation of weights to the `.coef_` of the `classifier` using the formula below: \n",
    "  - $w' = (1 - \\beta)\\bar{w} + \\beta w$ where $\\bar{w}$ is the mean magnitude of all the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict on a new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"chinese chinese chinese tokyo japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first must `vectorize` the test item into the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_original = vectorizer_bernoulli.transform([test_doc])\n",
    "test_features_original.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `NaiveBayesEnhancedClassifier.predict()` to (1) apply the `NB-transformation` and make our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.predict(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the \"confidence\" of this prediction using `NaiveBayesEnhancedClassifier.decision_function_predict_proba(test_features_original)` which will use `.decision_function()` or `.predict_proba()` of the original `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23337689])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.decision_function_predict_proba(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the test document to be in the `not China` class but not with much confidence.  \n",
    "\n",
    "Note: This is opposite the prediction made in the Stanford IR worked example because they use `multinomial NB` and the presence of \"China\" three times outweighed the presence of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `multiclass` test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, this transformation is built for `binary` classification.  The `multiclass` version requires the application of `one-v-all` to a traditional `SVM` classifier.  This is all done automatically within the `NaiveBayesEnhancedClassifier` `class`.  An example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obvious_docs = [\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "]\n",
    "\n",
    "obvious_labels = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    \"cat\",\n",
    "    \"pen\",\n",
    "    \"pencil\",\n",
    "    \"car\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traditional `bernoulli` `vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_bernoulli_vectorizer = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "multiclass_original_features = multiclass_bernoulli_vectorizer.fit_transform(obvious_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    # we'll use the default settings for `clf`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(base_clf=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_clf.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project the test documents into the same feature space as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_test_features = multiclass_bernoulli_vectorizer.transform(test_docs)\n",
    "multiclass_test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's get predictions for each test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_clf.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the classifier is generating predictions for how likely the data point is in that class.  We can see these values by calling `NaiveBayesEnhancedClassifier.decision_function_predict_proba()`.\n",
    "\n",
    "Below, each row is a classifier trained for a different label.  The values in the vector represent the margin of each data point from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.48603642 -0.4641514  -0.98429487 -1.06914084]\n",
      "1 [-0.98429487 -0.4641514   0.48603642 -1.06914084]\n",
      "2 [-0.98429487 -0.4641514  -0.98429487  0.64103137]\n"
     ]
    }
   ],
   "source": [
    "for i, row in zip([0,1,2], multiclass_example_clf.decision_function_predict_proba(multiclass_test_features)):\n",
    "    print(i, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the \"most confident\" classifier for each data point, where \"most confident\" is the one with the largest *positive* (or, if they're all *negative*, the *smallest*) margin.  In this case, each column is a data point, so we take the `argmax` of each column.\n",
    "\n",
    "Note: Since the second data point (\"pen\") was never seen in the training vocabulary, the classifiers all generate the same prediction, none of which are positive (implying a low level of confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `classifier` without `.coef_` or `.decision_function()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NaiveBayesEnhancedClassifier` is built to wrap *any* `sklearn` classifier.  If the classifier does *not* use weights that are captured in the attribute, `.coef_`, then `interpolation` is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_example_no_decision_function = NaiveBayesEnhancedClassifier(\n",
    "    base_clf=RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:190: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NaiveBayesEnhancedClassifier(base_clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "               interpolation_factor=0.25)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, if the `classifier` chosen does *not* have a `.decision_function()` method, then `.predict_proba()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n",
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n",
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.6, 0. , 0.6],\n",
       "       [0.1, 0.6, 0.6, 0.6],\n",
       "       [0.1, 0.6, 0. , 1. ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.decision_function_predict_proba(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each row still represents a `binary` classifier for a given label, and the vector values are the probabilities of the `positive` class.  We still choose the `argmax` in each column to determine our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n",
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n",
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:261: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on `20Newsgroups`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we attempt to recreate the `binary` classification problems in the experiments of the [original paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf).  Whenever provided, we use the same hyperparameters.\n",
    "\n",
    "We also compare different classifiers and the use of (1) no transformer or (2) `TfidfVectorizer()`.\n",
    "\n",
    "Note: It's likely that the data coming from `sklearn` for `20Newsgroups` is *not* exactly the same as the data used by the authors.  But the results are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `alt.atheism` v. `talk.religion.misc` (`AthR`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism', 'talk.religion.misc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 480)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<857x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136539 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<570x17440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86232 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ridge - nb transformed\n",
      "training perceptron - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training passive_aggressive - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random_forest - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed random_forest\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.8333333333333334),\n",
       "             ('sgd', 0.7964912280701755),\n",
       "             ('ridge', 0.7912280701754386),\n",
       "             ('perceptron', 0.7947368421052632),\n",
       "             ('passive_aggressive', 0.8385964912280702),\n",
       "             ('random_forest', 0.7789473684210526),\n",
       "             ('adaboost', 0.7596491228070176)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline ridge - no transformation\n",
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n",
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.7771929824561403),\n",
       "             ('sgd', 0.756140350877193),\n",
       "             ('ridge', 0.7701754385964912),\n",
       "             ('perceptron', 0.7666666666666667),\n",
       "             ('passive_aggressive', 0.7701754385964912),\n",
       "             ('random_forest', 0.7789473684210526),\n",
       "             ('adaboost', 0.7596491228070176),\n",
       "             ('bernoulli_nb', 0.8315789473684211),\n",
       "             ('multinomial_nb', 0.8368421052631579)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.7964912280701755),\n",
       "             ('sgd', 0.8122807017543859),\n",
       "             ('ridge', 0.8140350877192982),\n",
       "             ('perceptron', 0.7929824561403509),\n",
       "             ('passive_aggressive', 0.8210526315789474),\n",
       "             ('random_forest', 0.7543859649122807),\n",
       "             ('adaboost', 0.7421052631578947),\n",
       "             ('bernoulli_nb', 0.8315789473684211),\n",
       "             ('multinomial_nb', 0.8385964912280702)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "no_transform",
         "type": "bar",
         "uid": "cfbb29b3-b53f-4f54-8597-e186fee5a4ab",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.7771929824561403,
          0.756140350877193,
          0.7701754385964912,
          0.7666666666666667,
          0.7701754385964912,
          0.7789473684210526,
          0.7596491228070176,
          0.8315789473684211,
          0.8368421052631579
         ]
        },
        {
         "name": "tfidf_transform",
         "type": "bar",
         "uid": "d21e088d-38b4-43aa-a99a-49b890f35e57",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.7964912280701755,
          0.8122807017543859,
          0.8140350877192982,
          0.7929824561403509,
          0.8210526315789474,
          0.7543859649122807,
          0.7421052631578947,
          0.8315789473684211,
          0.8385964912280702
         ]
        },
        {
         "name": "nb_transform",
         "type": "bar",
         "uid": "99c27bf3-f6b7-4e03-88f5-b83aee61ed3f",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.8333333333333334,
          0.7964912280701755,
          0.7912280701754386,
          0.7947368421052632,
          0.8385964912280702,
          0.7789473684210526,
          0.7596491228070176,
          0,
          0
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "title": {
         "text": "Transformation comparison: AthR (2.9)"
        },
        "yaxis": {
         "range": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"40358d0c-046c-4c17-9ecb-7848a230d95a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\")) {\n",
       "    Plotly.newPlot(\"40358d0c-046c-4c17-9ecb-7848a230d95a\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7771929824561403, 0.756140350877193, 0.7701754385964912, 0.7666666666666667, 0.7701754385964912, 0.7789473684210526, 0.7596491228070176, 0.8315789473684211, 0.8368421052631579], \"type\": \"bar\", \"uid\": \"cfbb29b3-b53f-4f54-8597-e186fee5a4ab\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7964912280701755, 0.8122807017543859, 0.8140350877192982, 0.7929824561403509, 0.8210526315789474, 0.7543859649122807, 0.7421052631578947, 0.8315789473684211, 0.8385964912280702], \"type\": \"bar\", \"uid\": \"d21e088d-38b4-43aa-a99a-49b890f35e57\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8333333333333334, 0.7964912280701755, 0.7912280701754386, 0.7947368421052632, 0.8385964912280702, 0.7789473684210526, 0.7596491228070176, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"99c27bf3-f6b7-4e03-88f5-b83aee61ed3f\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: AthR (2.9)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\")) {window._Plotly.Plots.resize(document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"40358d0c-046c-4c17-9ecb-7848a230d95a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\")) {\n",
       "    Plotly.newPlot(\"40358d0c-046c-4c17-9ecb-7848a230d95a\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7771929824561403, 0.756140350877193, 0.7701754385964912, 0.7666666666666667, 0.7701754385964912, 0.7789473684210526, 0.7596491228070176, 0.8315789473684211, 0.8368421052631579], \"type\": \"bar\", \"uid\": \"cfbb29b3-b53f-4f54-8597-e186fee5a4ab\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.7964912280701755, 0.8122807017543859, 0.8140350877192982, 0.7929824561403509, 0.8210526315789474, 0.7543859649122807, 0.7421052631578947, 0.8315789473684211, 0.8385964912280702], \"type\": \"bar\", \"uid\": \"d21e088d-38b4-43aa-a99a-49b890f35e57\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8333333333333334, 0.7964912280701755, 0.7912280701754386, 0.7947368421052632, 0.8385964912280702, 0.7789473684210526, 0.7596491228070176, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"99c27bf3-f6b7-4e03-88f5-b83aee61ed3f\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: AthR (2.9)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\")) {window._Plotly.Plots.resize(document.getElementById(\"40358d0c-046c-4c17-9ecb-7848a230d95a\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: AthR (2.9)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `comp.graphics` v. `comp.windows.x` (`XGraph`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'comp.graphics', 'comp.windows.x',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'comp.graphics', 1: 'comp.windows.x'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 584)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1177x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 136868 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<784x21163 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x[0], idx2class_lookup.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ridge - nb transformed\n",
      "training perceptron - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training passive_aggressive - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random_forest - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed random_forest\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.8329081632653061),\n",
       "             ('sgd', 0.8367346938775511),\n",
       "             ('ridge', 0.8214285714285714),\n",
       "             ('perceptron', 0.8494897959183674),\n",
       "             ('passive_aggressive', 0.8469387755102041),\n",
       "             ('random_forest', 0.8061224489795918),\n",
       "             ('adaboost', 0.8112244897959183)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n",
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.8239795918367347),\n",
       "             ('sgd', 0.8482142857142857),\n",
       "             ('ridge', 0.8341836734693877),\n",
       "             ('perceptron', 0.8329081632653061),\n",
       "             ('passive_aggressive', 0.8290816326530612),\n",
       "             ('random_forest', 0.8137755102040817),\n",
       "             ('adaboost', 0.8112244897959183),\n",
       "             ('bernoulli_nb', 0.8176020408163265),\n",
       "             ('multinomial_nb', 0.8635204081632653)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.8482142857142857),\n",
       "             ('sgd', 0.8571428571428571),\n",
       "             ('ridge', 0.8545918367346939),\n",
       "             ('perceptron', 0.8431122448979592),\n",
       "             ('passive_aggressive', 0.8545918367346939),\n",
       "             ('random_forest', 0.8086734693877551),\n",
       "             ('adaboost', 0.7806122448979592),\n",
       "             ('bernoulli_nb', 0.8176020408163265),\n",
       "             ('multinomial_nb', 0.8673469387755102)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "no_transform",
         "type": "bar",
         "uid": "eef8cedd-0ded-49c2-8817-231dc184f4ce",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.8239795918367347,
          0.8482142857142857,
          0.8341836734693877,
          0.8329081632653061,
          0.8290816326530612,
          0.8137755102040817,
          0.8112244897959183,
          0.8176020408163265,
          0.8635204081632653
         ]
        },
        {
         "name": "tfidf_transform",
         "type": "bar",
         "uid": "fc7b5225-88a3-4e83-a335-24763142f18d",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.8482142857142857,
          0.8571428571428571,
          0.8545918367346939,
          0.8431122448979592,
          0.8545918367346939,
          0.8086734693877551,
          0.7806122448979592,
          0.8176020408163265,
          0.8673469387755102
         ]
        },
        {
         "name": "nb_transform",
         "type": "bar",
         "uid": "eec28add-4ed8-4a86-9458-7b90eacd9b58",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.8329081632653061,
          0.8367346938775511,
          0.8214285714285714,
          0.8494897959183674,
          0.8469387755102041,
          0.8061224489795918,
          0.8112244897959183,
          0,
          0
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "title": {
         "text": "Transformation comparison: XGraph (1.8)"
        },
        "yaxis": {
         "range": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"a2e6d5a9-2aac-4971-9420-9dd689561880\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\")) {\n",
       "    Plotly.newPlot(\"a2e6d5a9-2aac-4971-9420-9dd689561880\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8239795918367347, 0.8482142857142857, 0.8341836734693877, 0.8329081632653061, 0.8290816326530612, 0.8137755102040817, 0.8112244897959183, 0.8176020408163265, 0.8635204081632653], \"type\": \"bar\", \"uid\": \"eef8cedd-0ded-49c2-8817-231dc184f4ce\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8482142857142857, 0.8571428571428571, 0.8545918367346939, 0.8431122448979592, 0.8545918367346939, 0.8086734693877551, 0.7806122448979592, 0.8176020408163265, 0.8673469387755102], \"type\": \"bar\", \"uid\": \"fc7b5225-88a3-4e83-a335-24763142f18d\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8329081632653061, 0.8367346938775511, 0.8214285714285714, 0.8494897959183674, 0.8469387755102041, 0.8061224489795918, 0.8112244897959183, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"eec28add-4ed8-4a86-9458-7b90eacd9b58\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: XGraph (1.8)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\")) {window._Plotly.Plots.resize(document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a2e6d5a9-2aac-4971-9420-9dd689561880\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\")) {\n",
       "    Plotly.newPlot(\"a2e6d5a9-2aac-4971-9420-9dd689561880\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8239795918367347, 0.8482142857142857, 0.8341836734693877, 0.8329081632653061, 0.8290816326530612, 0.8137755102040817, 0.8112244897959183, 0.8176020408163265, 0.8635204081632653], \"type\": \"bar\", \"uid\": \"eef8cedd-0ded-49c2-8817-231dc184f4ce\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8482142857142857, 0.8571428571428571, 0.8545918367346939, 0.8431122448979592, 0.8545918367346939, 0.8086734693877551, 0.7806122448979592, 0.8176020408163265, 0.8673469387755102], \"type\": \"bar\", \"uid\": \"fc7b5225-88a3-4e83-a335-24763142f18d\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.8329081632653061, 0.8367346938775511, 0.8214285714285714, 0.8494897959183674, 0.8469387755102041, 0.8061224489795918, 0.8112244897959183, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"eec28add-4ed8-4a86-9458-7b90eacd9b58\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: XGraph (1.8)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\")) {window._Plotly.Plots.resize(document.getElementById(\"a2e6d5a9-2aac-4971-9420-9dd689561880\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: XGraph (1.8)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rec.sport.baseball` v. `sci.crypt` (`BbCrypt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.sport.baseball', 'sci.crypt',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'rec.sport.baseball', 1: 'sci.crypt'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595, 597)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0033557078469723042"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1192x21197 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 176313 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<793x21197 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 97031 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n",
      "training sgd - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ridge - nb transformed\n",
      "training perceptron - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training passive_aggressive - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random_forest - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training adaboost - nb transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py:165: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing transformed svm_from_paper\n",
      "testing transformed sgd\n",
      "testing transformed ridge\n",
      "testing transformed perceptron\n",
      "testing transformed passive_aggressive\n",
      "testing transformed random_forest\n",
      "testing transformed adaboost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.9621689785624212),\n",
       "             ('sgd', 0.9684741488020177),\n",
       "             ('ridge', 0.9432534678436317),\n",
       "             ('perceptron', 0.9810844892812106),\n",
       "             ('passive_aggressive', 0.969735182849937),\n",
       "             ('random_forest', 0.9432534678436317),\n",
       "             ('adaboost', 0.9319041614123581)])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - no transformation\n",
      "training baseline sgd - no transformation\n",
      "training baseline ridge - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline perceptron - no transformation\n",
      "training baseline passive_aggressive - no transformation\n",
      "training baseline random_forest - no transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - no transformation\n",
      "training baseline bernoulli_nb - no transformation\n",
      "training baseline multinomial_nb - no transformation\n"
     ]
    }
   ],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.9583858764186634),\n",
       "             ('sgd', 0.9558638083228247),\n",
       "             ('ridge', 0.9470365699873896),\n",
       "             ('perceptron', 0.9520807061790668),\n",
       "             ('passive_aggressive', 0.9596469104665826),\n",
       "             ('random_forest', 0.9495586380832283),\n",
       "             ('adaboost', 0.9319041614123581),\n",
       "             ('bernoulli_nb', 0.9470365699873896),\n",
       "             ('multinomial_nb', 0.9861286254728878)])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline svm_from_paper - tfidf transformation\n",
      "training baseline sgd - tfidf transformation\n",
      "training baseline ridge - tfidf transformation\n",
      "training baseline perceptron - tfidf transformation\n",
      "training baseline passive_aggressive - tfidf transformation\n",
      "training baseline random_forest - tfidf transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:332: UserWarning:\n",
      "\n",
      "In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n",
      "C:\\Users\\michael.capizzi\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning:\n",
      "\n",
      "n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline adaboost - tfidf transformation\n",
      "training baseline bernoulli_nb - tfidf transformation\n",
      "training baseline multinomial_nb - tfidf transformation\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing baseline svm_from_paper - no transformation\n",
      "testing baseline sgd - no transformation\n",
      "testing baseline ridge - no transformation\n",
      "testing baseline perceptron - no transformation\n",
      "testing baseline passive_aggressive - no transformation\n",
      "testing baseline random_forest - no transformation\n",
      "testing baseline adaboost - no transformation\n",
      "testing baseline bernoulli_nb - no transformation\n",
      "testing baseline multinomial_nb - no transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm_from_paper', 0.9520807061790668),\n",
       "             ('sgd', 0.9634300126103404),\n",
       "             ('ridge', 0.9709962168978562),\n",
       "             ('perceptron', 0.9558638083228247),\n",
       "             ('passive_aggressive', 0.9747793190416141),\n",
       "             ('random_forest', 0.9495586380832283),\n",
       "             ('adaboost', 0.9129886506935687),\n",
       "             ('bernoulli_nb', 0.9470365699873896),\n",
       "             ('multinomial_nb', 0.9798234552332913)])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "no_transform",
         "type": "bar",
         "uid": "5df2524c-27bf-45e5-9ba5-8a968570e75d",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.9583858764186634,
          0.9558638083228247,
          0.9470365699873896,
          0.9520807061790668,
          0.9596469104665826,
          0.9495586380832283,
          0.9319041614123581,
          0.9470365699873896,
          0.9861286254728878
         ]
        },
        {
         "name": "tfidf_transform",
         "type": "bar",
         "uid": "d94643df-2916-4be5-a0bc-df73d3e385f1",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.9520807061790668,
          0.9634300126103404,
          0.9709962168978562,
          0.9558638083228247,
          0.9747793190416141,
          0.9495586380832283,
          0.9129886506935687,
          0.9470365699873896,
          0.9798234552332913
         ]
        },
        {
         "name": "nb_transform",
         "type": "bar",
         "uid": "04a73270-7130-425e-a5ee-5d3955afbaa6",
         "x": [
          "svm_from_paper",
          "sgd",
          "ridge",
          "perceptron",
          "passive_aggressive",
          "random_forest",
          "adaboost",
          "bernoulli_nb",
          "multinomial_nb"
         ],
         "y": [
          0.9621689785624212,
          0.9684741488020177,
          0.9432534678436317,
          0.9810844892812106,
          0.969735182849937,
          0.9432534678436317,
          0.9319041614123581,
          0,
          0
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "title": {
         "text": "Transformation comparison: BbCrypt (0.5)"
        },
        "yaxis": {
         "range": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\")) {\n",
       "    Plotly.newPlot(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9583858764186634, 0.9558638083228247, 0.9470365699873896, 0.9520807061790668, 0.9596469104665826, 0.9495586380832283, 0.9319041614123581, 0.9470365699873896, 0.9861286254728878], \"type\": \"bar\", \"uid\": \"5df2524c-27bf-45e5-9ba5-8a968570e75d\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9520807061790668, 0.9634300126103404, 0.9709962168978562, 0.9558638083228247, 0.9747793190416141, 0.9495586380832283, 0.9129886506935687, 0.9470365699873896, 0.9798234552332913], \"type\": \"bar\", \"uid\": \"d94643df-2916-4be5-a0bc-df73d3e385f1\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9621689785624212, 0.9684741488020177, 0.9432534678436317, 0.9810844892812106, 0.969735182849937, 0.9432534678436317, 0.9319041614123581, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"04a73270-7130-425e-a5ee-5d3955afbaa6\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: BbCrypt (0.5)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\")) {window._Plotly.Plots.resize(document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\")) {\n",
       "    Plotly.newPlot(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\", [{\"name\": \"no_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9583858764186634, 0.9558638083228247, 0.9470365699873896, 0.9520807061790668, 0.9596469104665826, 0.9495586380832283, 0.9319041614123581, 0.9470365699873896, 0.9861286254728878], \"type\": \"bar\", \"uid\": \"5df2524c-27bf-45e5-9ba5-8a968570e75d\"}, {\"name\": \"tfidf_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9520807061790668, 0.9634300126103404, 0.9709962168978562, 0.9558638083228247, 0.9747793190416141, 0.9495586380832283, 0.9129886506935687, 0.9470365699873896, 0.9798234552332913], \"type\": \"bar\", \"uid\": \"d94643df-2916-4be5-a0bc-df73d3e385f1\"}, {\"name\": \"nb_transform\", \"x\": [\"svm_from_paper\", \"sgd\", \"ridge\", \"perceptron\", \"passive_aggressive\", \"random_forest\", \"adaboost\", \"bernoulli_nb\", \"multinomial_nb\"], \"y\": [0.9621689785624212, 0.9684741488020177, 0.9432534678436317, 0.9810844892812106, 0.969735182849937, 0.9432534678436317, 0.9319041614123581, 0.0, 0.0], \"type\": \"bar\", \"uid\": \"04a73270-7130-425e-a5ee-5d3955afbaa6\"}], {\"barmode\": \"group\", \"title\": {\"text\": \"Transformation comparison: BbCrypt (0.5)\"}, \"yaxis\": {\"range\": [0, 1]}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\")) {window._Plotly.Plots.resize(document.getElementById(\"0e60d84e-727a-4b50-9a1c-8d7f69fc11af\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: BbCrypt (0.5)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `20Newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism',\n",
       " 1: 'comp.graphics',\n",
       " 2: 'comp.os.ms-windows.misc',\n",
       " 3: 'comp.sys.ibm.pc.hardware',\n",
       " 4: 'comp.sys.mac.hardware',\n",
       " 5: 'comp.windows.x',\n",
       " 6: 'misc.forsale',\n",
       " 7: 'rec.autos',\n",
       " 8: 'rec.motorcycles',\n",
       " 9: 'rec.sport.baseball',\n",
       " 10: 'rec.sport.hockey',\n",
       " 11: 'sci.crypt',\n",
       " 12: 'sci.electronics',\n",
       " 13: 'sci.med',\n",
       " 14: 'sci.space',\n",
       " 15: 'soc.religion.christian',\n",
       " 16: 'talk.politics.guns',\n",
       " 17: 'talk.politics.mideast',\n",
       " 18: 'talk.politics.misc',\n",
       " 19: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x125145 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1604644 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7532x125145 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 989304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training svm_from_paper - nb transformed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-fe39526b793f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewsgroups_nb_plus_clfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training {} - nb transformed\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewsgroups_train_features_uni\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\projects\\naive-bayes-transformer\\nb_transformer\\nb_enhanced_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_transformers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;31m# fit individual classifier with transformed data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# update weights with interpolation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"crammer_singer\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    924\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: 20Newsgroups',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
