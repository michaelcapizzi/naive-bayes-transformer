{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SVM` with `NB` features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements shows how to use the NB-SVM classifier from [this paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) that has become a classic (and competitive) baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy dataset (binary)\n",
    "\n",
    "Working example borrowed from [Stanford IR online text](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "\n",
    "| docID | text                                | label     |\n",
    "|-------|-------------------------------------|-----------|\n",
    "| 1     | chinese beijing chinese             | CHINA     |\n",
    "| 2     | chinese chinese shanghai            | CHINA     |\n",
    "| 3     | chinese macao                       | CHINA     |\n",
    "| 4     | tokyo japan chinese                 | NOT_CHINA |\n",
    "| 5     | chinese chinese chinese tokyo japan | ???       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_one = \"chinese beijing chinese\"\n",
    "label_one = 1\n",
    "\n",
    "doc_two = \"chinese chinese shanghai\"\n",
    "label_two = 1\n",
    "\n",
    "doc_three = \"chinese macao\"\n",
    "label_three = 1\n",
    "\n",
    "doc_four = \"tokyo japan chinese\"\n",
    "label_four = 0\n",
    "\n",
    "all_docs = [doc_one, doc_two, doc_three, doc_four]\n",
    "all_labels = [label_one, label_two, label_three, label_four]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0, os.path.join(parentdir, 'nb_transformer'))\n",
    "from nb_transformer import NaiveBayesTransformer\n",
    "from nb_enhanced_classifier import NaiveBayesEnhancedClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional `bernoulli` `vectorizer`\n",
    "\n",
    "First let's `vectorize` the documents in the training set using `CountVectorizer()`. <br>\n",
    "The paper found binary features to be more effective than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_bernoulli = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "X_bernoulli = vectorizer_bernoulli.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beijing': 0, 'chinese': 1, 'japan': 2, 'macao': 3, 'shanghai': 4, 'tokyo': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bernoulli.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bernoulli.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NaiveBayesTransformer`\n",
    "\n",
    "This is a custom `transformer` that will use the Naive Bayes conditional probabilities as feature values.  See [the paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) for details.\n",
    "\n",
    "The key thing to recognize is that this method is built off of the assumption of a `binary` (only two labels) problem.  It can be expanded to `multiclass` utilizing the `one-v-all` approach, but this also requires a **different** transformation for **each** label, thus the inclusion of `label_of_interest` as an argument to the `NaiveBayesTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_transformer = NaiveBayesTransformer(all_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = vectorizer_bernoulli.fit_transform(all_docs)\n",
    "original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4054651 ,  0.4054651 , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.        ,  0.4054651 ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.        ,  0.4054651 ,  0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.4054651 , -0.98082924,  0.        ,  0.        ,\n",
       "        -0.98082924]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_transformed = nb_transformer.fit_transform(original_features)\n",
    "nb_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our features, let's train a classifier to predict `1` v. `0`. \n",
    "The paper found an `SVM` with `squared loss` and `l2` penalizing to be most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(            \n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state = 1,         # to ensure reproducible results\n",
    "    class_weight='balanced',\n",
    "    dual=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an instance of `NaiveBayesEnchancedClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 1,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    clf=svm_clf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it on our dataset by feeding the *original* vectorized features into `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_example_clf.fit(original_features, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baked into `.fit()` are the following steps:\n",
    " 1. `transform` the features using the `NaiveBayesTransformer`\n",
    " 2. Apply an interpolation of weights to the `.coef_` of the `classifier` using the formula below: \n",
    "  - $w' = (1 - \\beta)\\bar{w} + \\beta w$ where $\\bar{w}$ is the mean magnitude of all the weights\n",
    "  \n",
    "Below are a few cells showing the calculations that are done in `NaiveBayesTransformer._interpolate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3158278 ,  0.32634704,  0.45491727,  0.3158278 ,  0.3158278 ,\n",
       "         0.45491727]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolation_factor = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36394416,  0.36394416,  0.36394416,  0.36394416,  0.36394416,\n",
       "         0.36394416]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_magnitude = np.sum(svm_clf.coef_)/svm_clf.coef_.shape[1]\n",
    "mean_magnitude_vector = np.full(svm_clf.coef_.shape, mean_magnitude)\n",
    "mean_magnitude_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35191507,  0.35454488,  0.38668744,  0.35191507,  0.35191507,\n",
       "         0.38668744]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_ = (1 - interpolation_factor) * mean_magnitude_vector + \\\n",
    "                                interpolation_factor * svm_clf.coef_\n",
    "svm_clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict on a new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"chinese chinese chinese tokyo japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first must `vectorize` the test item into the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features_original = vectorizer_bernoulli.transform([test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `NaiveBayesEnhancedClassifier.predict()` to (1) apply the `NB-transformation` and make our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.predict(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the \"confidence\" of this prediction using `NaiveBayesEnhancedClassifier.decision_function_predict_proba(test_features_original)` which will use `.decision_function()` or `.predict_proba()` of the original `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08810002])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_example_clf.decision_function_predict_proba(test_features_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the test document to be in the `not China` class but not with much confidence.  \n",
    "\n",
    "Note: This is opposite the prediction made in the Stanford IR worked example because they use `multinomial NB` and the presence of \"China\" three times outweighed the presence of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `multiclass` test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, this transformation is built for `binary` classification.  The `multiclass` version requires the application of `one-v-all` to a traditional `SVM` classifier.  This is all done automatically within the `NaiveBayesEnhancedClassifier` `class`.  An example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obvious_docs = [\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"cat\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"pencil\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "    \"car\",\n",
    "]\n",
    "\n",
    "obvious_labels = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    \"cat\",\n",
    "    \"pen\",\n",
    "    \"pencil\",\n",
    "    \"car\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traditional `bernoulli` `vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_bernoulli_vectorizer = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "multiclass_original_features = multiclass_bernoulli_vectorizer.fit_transform(obvious_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_original_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_example_clf = NaiveBayesEnhancedClassifier(\n",
    "    # we'll use the default settings for `clf`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_example_clf.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project the test documents into the same feature space as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_test_features = multiclass_bernoulli_vectorizer.transform(test_docs)\n",
    "multiclass_test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's get predictions for each test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_clf.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the classifier is generating predictions for how likely the data point is in that class.  We can see these values by calling `NaiveBayesEnhancedClassifier.decision_function_predict_proba()`.\n",
    "\n",
    "Below, each row is a classifier trained for a different label.  The values in the vector represent the margin of each data point from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.48603642 -0.4641514  -0.98429487 -1.06914084]\n",
      "1 [-0.98429487 -0.4641514   0.48603642 -1.06914084]\n",
      "2 [-0.98429487 -0.4641514  -0.98429487  0.64103137]\n"
     ]
    }
   ],
   "source": [
    "for i, row in zip([0,1,2], multiclass_example_clf.decision_function_predict_proba(multiclass_test_features)):\n",
    "    print(i, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the \"most confident\" classifier for each data point, where \"most confident\" is the one with the largest *positive* (or, if they're all *negative*, the *smallest*) margin.  In this case, each column is a data point, so we take the `argmax` of each column.\n",
    "\n",
    "Note: Since the second data point (\"pen\") was never seen in the training vocabulary, the classifiers all generate the same prediction, none of which are positive (implying a low level of confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `classifier` without `.coef_` or `.decision_function()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NaiveBayesEnhancedClassifier` is built to wrap *any* `sklearn` classifier.  If the classifier does *not* use weights that are captured in the attribute, `.coef_`, then `interpolation` is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_example_no_decision_function = NaiveBayesEnhancedClassifier(\n",
    "    clf=RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:172: UserWarning:\n",
      "\n",
      "the classifier you instantiated does not have an attribute `.coef_`; interpolation will not occur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.fit(\n",
    "    multiclass_bernoulli_vectorizer.fit_transform(obvious_docs),   # vectorize the original documents\n",
    "    obvious_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, if the `classifier` chosen does *not* have a `.decision_function()` method, then `.predict_proba()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:234: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3,  0.3,  0. ,  0.3],\n",
       "       [ 0. ,  0.3,  0.3,  0.3],\n",
       "       [ 0. ,  0.3,  0. ,  1. ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.decision_function_predict_proba(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each row still represents a `binary` classifier for a given label, and the vector values are the probabilities of the `positive` class.  We still choose the `argmax` in each column to determine our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mcapizzi/data/Github/naive-bayes-transformer/nb_transformer/nb_enhanced_classifier.py:234: UserWarning:\n",
      "\n",
      "classifier you instantiated does not have a method, `decision_function()`;using `predict_proba()` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_example_no_decision_function.predict(multiclass_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on `20Newsgroups`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we attempt to recreate the `binary` classification problems in the experiments of the [original paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf).  Whenever provided, we use the same hyperparameters.\n",
    "\n",
    "We also compare different classifiers and the use of (1) no transformer or (2) `TfidfVectorizer()`.\n",
    "\n",
    "Note: It's likely that the data coming from `sklearn` for `20Newsgroups` is *not* exactly the same as the data used by the authors.  But the results are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `alt.atheism` v. `talk.religion.misc` (`AthR`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism', 'talk.religion.misc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove = (\n",
    "    'headers'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: AthR (2.9)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `comp.graphics` v. `comp.windows.x` (`XGraph`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'comp.graphics', 'comp.windows.x',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: XGraph (1.8)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rec.sport.baseball` v. `sci.crypt` (`BbCrypt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.sport.baseball', 'sci.crypt',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label distribution\n",
    "num_pos = list(data_train.target).count(1)\n",
    "num_neg = list(data_train.target).count(0)\n",
    "num_pos, num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine intercept (log(N+/N-))\n",
    "intercept = np.log(num_pos/num_neg)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: BbCrypt (0.5)',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `20Newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "idx2class_lookup = dict((i,c) for i,c in enumerate(data_train.target_names))\n",
    "idx2class_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_vectorizer_uni = CountVectorizer(\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "newsgroups_vectorizer_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_uni.fit_transform(data_train.data)\n",
    "newsgroups_train_features_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_features_uni = newsgroups_vectorizer_uni.transform(data_test.data)\n",
    "newsgroups_test_features_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformed - nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    random_state=1,\n",
    "    dual=False\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_nb_plus_clfs = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    newsgroups_nb_plus_clfs[clf_name] = NaiveBayesEnhancedClassifier(\n",
    "        list(map(lambda x: x[0], idx2class_lookup.items())),\n",
    "        clf,\n",
    "        interpolation_factor=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed - nb\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"training {} - nb transformed\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in newsgroups_nb_plus_clfs.items():\n",
    "    print(\"testing transformed {}\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_nb_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_nb_transformed_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    C=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01,\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transformation count\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - no transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_no_transformation_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_no_transformation_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_no_transformation_uni_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train_features_uni = newsgroups_vectorizer_tfidf.fit_transform(data_train.data)\n",
    "newsgroups_test_features_uni = newsgroups_vectorizer_tfidf.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = OrderedDict()\n",
    "\n",
    "all_classifiers[\"svm_from_paper\"] = LinearSVC(\n",
    "    loss='squared_hinge',\n",
    "    penalty='l2',\n",
    "    C=0.1,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"sgd\"] = SGDClassifier(\n",
    "    n_iter=50,\n",
    "    penalty='elasticnet',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"ridge\"] = RidgeClassifier(\n",
    "    tol=1e-2,\n",
    "    solver='lsqr',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"perceptron\"] = Perceptron(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"passive_aggressive\"] = PassiveAggressiveClassifier(\n",
    "    n_iter=50,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"random_forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"adaboost\"] = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "all_classifiers[\"bernoulli_nb\"] = BernoulliNB(\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "all_classifiers[\"multinomial_nb\"] = MultinomialNB(\n",
    "    alpha=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"training baseline {} - tfidf transformation\".format(clf_name))\n",
    "    clf.fit(newsgroups_train_features_uni, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_tfidf_transformed_uni_results = OrderedDict()\n",
    "for clf_name, clf in all_classifiers.items():\n",
    "    print(\"testing baseline {} - no transformation\".format(clf_name))\n",
    "    preds = clf.predict(newsgroups_test_features_uni)\n",
    "    newsgroups_tfidf_transformed_uni_results[clf_name] = accuracy_score(data_test.target, preds)\n",
    "newsgroups_tfidf_transformed_uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (\"no_transform\", newsgroups_no_transformation_uni_results),\n",
    "    (\"tfidf_transform\", newsgroups_tfidf_transformed_uni_results), \n",
    "    (\"nb_transform\", newsgroups_nb_transformed_uni_results)\n",
    "]\n",
    "\n",
    "clfs = list(all_classifiers.keys())\n",
    "\n",
    "traces = OrderedDict()\n",
    "for i in range(len(groups)):\n",
    "    transformation = groups[i][1]\n",
    "    transformation_name = groups[i][0]\n",
    "    scores = []\n",
    "    for label in clfs:\n",
    "        try:\n",
    "            score = transformation[label]\n",
    "        except:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "    traces[\"trace_{}\".format(transformation_name)] = go.Bar(\n",
    "        x=clfs,\n",
    "        y=scores,\n",
    "        name=transformation_name,\n",
    "    )\n",
    "\n",
    "data_ = [v for k, v in traces.items()]\n",
    "layout_ = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Transformation comparison: 20Newsgroups',\n",
    "    yaxis=dict(\n",
    "        range=[0, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_ = go.Figure(data=data_, layout=layout_)\n",
    "iplot(fig_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
